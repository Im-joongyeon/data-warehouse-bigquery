# Data Warehouse Setup Guide

ì´ ê°€ì´ë“œëŠ” Kafkaì—ì„œ BigQueryë¡œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### ì™„ë£Œ í•„ìˆ˜
- [x] data-ingestion íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘
- [x] GCP í”„ë¡œì íŠ¸ ìƒì„±
- [x] BigQuery API í™œì„±í™”
- [x] Service Account ìƒì„± ë° JSON í‚¤ ë‹¤ìš´ë¡œë“œ
- [x] Service Accountì— BigQuery ê¶Œí•œ ë¶€ì—¬

### ì†Œí”„íŠ¸ì›¨ì–´
- Docker & Docker Compose
- gcloud CLI (ì„ íƒì‚¬í•­, BigQuery ìˆ˜ë™ ì„¤ì • ì‹œ)
- bq CLI (ì„ íƒì‚¬í•­, gcloudì— í¬í•¨)

## ğŸš€ Step 1: GCP Service Account í‚¤ ì„¤ì •

### 1-1. JSON í‚¤ íŒŒì¼ ë³µì‚¬

ë‹¤ìš´ë¡œë“œí•œ Service Account JSON í‚¤ë¥¼ `gcp/` ë””ë ‰í† ë¦¬ì— ë³µì‚¬:

```bash
# Downloads í´ë”ì—ì„œ ë³µì‚¬ (íŒŒì¼ëª…ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
cp ~/Downloads/í”„ë¡œì íŠ¸id.json gcp/service-account-key.json

# ê¶Œí•œ ì„¤ì • (ë³´ì•ˆ)
chmod 600 gcp/service-account-key.json
```

### 1-2. í‚¤ íŒŒì¼ ê²€ì¦

```bash
# JSON í˜•ì‹ í™•ì¸
cat gcp/service-account-key.json | jq '.project_id'
# ì¶œë ¥: "í”„ë¡œì íŠ¸id"

# íŒŒì¼ì´ .gitignoreì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
git status
# service-account-key.jsonì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•„ì•¼ í•¨
```

## ğŸ—„ï¸ Step 2: BigQuery ë°ì´í„°ì…‹ ë° í…Œì´ë¸” ìƒì„±

### ë°©ë²• 1: ìë™ ìŠ¤í¬ë¦½íŠ¸ (ì¶”ì²œ)

```bash
# gcloud ì¸ì¦ (ì²˜ìŒ í•œ ë²ˆë§Œ)
gcloud auth login

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
cd gcp
chmod +x setup-bigquery.sh
./setup-bigquery.sh
```

### ë°©ë²• 2: ìˆ˜ë™ ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ì„¤ì •
gcloud config set project í”„ë¡œì íŠ¸id

# ë°ì´í„°ì…‹ ìƒì„±
bq mk --dataset --location=US í”„ë¡œì íŠ¸id:kafka_ingestion

# í…Œì´ë¸” ìƒì„±
bq mk --table í”„ë¡œì íŠ¸id:kafka_ingestion.accounts schemas/accounts_schema.json
bq mk --table í”„ë¡œì íŠ¸id:kafka_ingestion.transactions schemas/transactions_schema.json

# í™•ì¸
bq ls í”„ë¡œì íŠ¸id:kafka_ingestion
```

### ë°©ë²• 3: BigQuery Console (ì›¹)

1. https://console.cloud.google.com/bigquery?project=í”„ë¡œì íŠ¸id
2. í”„ë¡œì íŠ¸ ì´ë¦„ ì˜† â‹® í´ë¦­ â†’ **ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê¸°**
3. ë°ì´í„° ì„¸íŠ¸ ID: `kafka_ingestion`
4. ìœ„ì¹˜: `US` (ë˜ëŠ” ì„ í˜¸í•˜ëŠ” ë¦¬ì „)
5. **ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê¸°** í´ë¦­

í…Œì´ë¸”ì€ Connectorê°€ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ (`autoCreateTables: true` ì„¤ì •).

## ğŸ³ Step 3: Docker Composeë¡œ Kafka Connect ì‹œì‘

### 3-1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
cp .env.example .env
# .env íŒŒì¼ í™•ì¸ (ê¸°ë³¸ê°’ìœ¼ë¡œ ì¶©ë¶„)
```

### 3-2. ì„œë¹„ìŠ¤ ì‹œì‘

```bash
docker-compose up -d
```

### 3-3. ì„œë¹„ìŠ¤ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker-compose ps

# Kafka Connect ë¡œê·¸ í™•ì¸
docker-compose logs -f connect-bigquery

# BigQuery connector plugin ì„¤ì¹˜ í™•ì¸ (60ì´ˆ ì •ë„ ì†Œìš”)
docker-compose logs connect-bigquery | grep "BigQuery"
```

**ì¤‘ìš”:** Kafka Connectê°€ ì™„ì „íˆ ì‹œì‘ë˜ê³  BigQuery connector pluginì´ ì„¤ì¹˜ë  ë•Œê¹Œì§€ **60-90ì´ˆ** ê¸°ë‹¤ë ¤ì•¼ í•©ë‹ˆë‹¤.

### 3-4. Health Check

```bash
# Kafka Connect API í™•ì¸
curl http://localhost:8084/

# Connector plugins í™•ì¸
curl http://localhost:8084/connector-plugins | jq '.' | grep BigQuery
```

`BigQuerySinkConnector`ê°€ ë³´ì´ë©´ ì¤€ë¹„ ì™„ë£Œ!

## ğŸ”Œ Step 4: BigQuery Sink Connector ë“±ë¡

### 4-1. data-ingestion íŒŒì´í”„ë¼ì¸ í™•ì¸

BigQueryë¡œ ì „ì†¡í•  Kafka í† í”½ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸:

```bash
# data-ingestion ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ../data-ingestion

# í† í”½ í™•ì¸
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092 | grep dbserver1

# ë©”ì‹œì§€ í™•ì¸
docker-compose exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic dbserver1.public.accounts \
  --max-messages 1

# data-warehouse ë””ë ‰í† ë¦¬ë¡œ ë³µê·€
cd ../data-warehouse
```

### 4-2. Connector ë“±ë¡

```bash
cd kafka-bigquery-connector
chmod +x register_sink.sh
./register_sink.sh
```

### 4-3. Connector ìƒíƒœ í™•ì¸

```bash
# accounts connector ìƒíƒœ
curl http://localhost:8084/connectors/bigquery-sink-accounts/status | jq '.'

# transactions connector ìƒíƒœ
curl http://localhost:8084/connectors/bigquery-sink-transactions/status | jq '.'
```

**ì •ìƒ ìƒíƒœ:**
```json
{
  "name": "bigquery-sink-accounts",
  "connector": {
    "state": "RUNNING",
    "worker_id": "..."
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "..."
    }
  ]
}
```

## âœ… Step 5: ë°ì´í„° ê²€ì¦

### 5-1. BigQuery Consoleì—ì„œ í™•ì¸

1. https://console.cloud.google.com/bigquery?project=í”„ë¡œì íŠ¸id
2. ì¢Œì¸¡ íƒìƒ‰ê¸°ì—ì„œ `í”„ë¡œì íŠ¸id` â†’ `kafka_ingestion` í™•ì¥
3. `accounts` í…Œì´ë¸” í´ë¦­ â†’ **ë¯¸ë¦¬ë³´ê¸°** íƒ­

### 5-2. SQL ì¿¼ë¦¬ë¡œ í™•ì¸

BigQuery Editorì—ì„œ ì‹¤í–‰:

```sql
-- accounts ë°ì´í„° í™•ì¸
SELECT * FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.accounts`
ORDER BY created_at DESC
LIMIT 10;

-- transactions ë°ì´í„° í™•ì¸
SELECT * FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.transactions`
ORDER BY created_at DESC
LIMIT 10;

-- ë°ì´í„° ê°œìˆ˜ í™•ì¸
SELECT 
  'accounts' as table_name,
  COUNT(*) as row_count
FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.accounts`
UNION ALL
SELECT 
  'transactions' as table_name,
  COUNT(*) as row_count
FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.transactions`;
```

### 5-3. ì‹¤ì‹œê°„ ë°ì´í„° í…ŒìŠ¤íŠ¸

**í„°ë¯¸ë„ 1 - BigQuery ëª¨ë‹ˆí„°ë§:**

BigQuery Consoleì—ì„œ ìë™ ìƒˆë¡œê³ ì¹¨ í™œì„±í™” ë˜ëŠ” ì¿¼ë¦¬ ë°˜ë³µ ì‹¤í–‰

**í„°ë¯¸ë„ 2 - PostgreSQL ë°ì´í„° ì‚½ì…:**

```bash
cd ../data-ingestion
docker-compose exec postgres psql -U postgres -d mydb

-- ìƒˆ ê³„ì¢Œ ìƒì„±
INSERT INTO accounts (user_id, balance, status) 
VALUES (1000, 100000.00, 'ACTIVE');

-- ê±°ë˜ ì¶”ê°€
INSERT INTO transactions (account_id, tx_type, amount, balance_after, status) 
VALUES (1000, 'deposit', 100000.00, 100000.00, 'COMPLETED');
```

**í„°ë¯¸ë„ 3 - BigQuery í™•ì¸ (10-30ì´ˆ í›„):**

```sql
SELECT * FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.accounts`
WHERE user_id = 1000;

SELECT * FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.transactions`
WHERE account_id = 1000;
```

## ğŸ” Step 6: ëª¨ë‹ˆí„°ë§

### Connector ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
docker-compose logs -f connect-bigquery

# ì—ëŸ¬ë§Œ í™•ì¸
docker-compose logs connect-bigquery | grep -i error

# íŠ¹ì • connector ë¡œê·¸
docker-compose logs connect-bigquery | grep bigquery-sink-accounts
```

### Connector ë©”íŠ¸ë¦­

```bash
# ëª¨ë“  connector ëª©ë¡
curl http://localhost:8084/connectors

# íŠ¹ì • connector ìƒì„¸ ì •ë³´
curl http://localhost:8084/connectors/bigquery-sink-accounts | jq '.'

# Task ìƒíƒœ
curl http://localhost:8084/connectors/bigquery-sink-accounts/tasks/0/status | jq '.'
```

### BigQuery ëª¨ë‹ˆí„°ë§

```sql
-- í…Œì´ë¸” í¬ê¸° ë° í–‰ ìˆ˜
SELECT 
  table_name,
  row_count,
  ROUND(size_bytes / 1024 / 1024, 2) as size_mb,
  ROUND(size_bytes / row_count, 0) as avg_row_bytes
FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.__TABLES__`;

-- ìµœê·¼ ì‚½ì… ì‹œê°„
SELECT 
  'accounts' as table_name,
  MAX(created_at) as last_insert_time
FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.accounts`
UNION ALL
SELECT 
  'transactions' as table_name,
  MAX(created_at) as last_insert_time
FROM `í”„ë¡œì íŠ¸id.kafka_ingestion.transactions`;
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ë°ì´í„°ê°€ BigQueryì— ì •ìƒì ìœ¼ë¡œ ë“¤ì–´ì˜¤ê³  ìˆë‹¤ë©´:

1. **dbt-project**: ë°ì´í„° ë³€í™˜ ë° ëª¨ë¸ë§
   - Staging models
   - Fact & Dimension tables
   - Data quality tests

2. **data-analytics**: ì‹œê°í™”
   - Looker Studio ëŒ€ì‹œë³´ë“œ
   - ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
   - ì•Œë¦¼ ì„¤ì •

## ğŸ“š ì°¸ê³  ìë£Œ

- [Kafka Connect BigQuery Sink](https://github.com/confluentinc/kafka-connect-bigquery)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [Confluent Hub](https://www.confluent.io/hub/confluentinc/kafka-connect-bigquery)
